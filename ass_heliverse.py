# -*- coding: utf-8 -*-
"""Ass_Heliverse.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EAme4VA10ESci5f0jaXiN3L9G611kS7X
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('/content/WA_Fn-UseC_-HR-Employee-Attrition.csv', encoding='latin1')

df.head()

df.shape

Gender = {'Female': 0, 'Male': 1}
df['Gender'] = df['Gender'].map(Gender)

df.dropna(inplace=True)

Attrition_mapping = {'No': 0, 'Yes': 1}
df['Attrition'] = df['Attrition'].map(Attrition_mapping)
X = df.drop(columns=['Attrition'])
y = df['Attrition']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

X_train_encoded = pd.get_dummies(X_train)
X_test_encoded = pd.get_dummies(X_test)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)

from sklearn.decomposition import PCA

pca = PCA(n_components=10)  # You can adjust the number of components as needed
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print("Explained variance ratio:", pca.explained_variance_ratio_)

import matplotlib.pyplot as plt

plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance Ratio of Principal Components')
plt.show()

import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import StackingClassifier, HistGradientBoostingClassifier
from sklearn.decomposition import PCA
import xgboost as xgb

base_classifiers = [
    ('LR', LogisticRegression()),
    ('KNN', KNeighborsClassifier()),
    ('DT', DecisionTreeClassifier()),
    ('MLP', MLPClassifier()),
    ('SVM', SVC(probability=True))
]

accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []

for clf_name, clf in base_classifiers:
    # Train classifier
    clf.fit(X_train_pca, y_train)

    # Predictions
    y_pred = clf.predict(X_test_pca)

    # Evaluate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    # Append metrics to lists
    accuracy_scores.append(accuracy)
    precision_scores.append(precision)
    recall_scores.append(recall)
    f1_scores.append(f1)

    # Print results
    print(f"Classifier: {clf_name}")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")
    print("\n")

base_accuracy = {}
for clf_name, clf in base_classifiers:
    clf.fit(X_train_pca, y_train)
    y_pred = clf.predict(X_test_pca)
    base_accuracy[clf_name] = accuracy_score(y_test, y_pred)

plt.figure(figsize=(15, 5))

# Bar graph for base classifier
base_colors = ['red', 'blue', 'green', 'orange', 'purple']
base_x = np.arange(len(base_accuracy))
plt.bar(base_x, base_accuracy.values(), color=base_colors,width=0.5)
plt.xticks(base_x, base_accuracy.keys())
for i, acc in enumerate(base_accuracy.values()):
    plt.text(i, acc + 0.01, f"{acc:.4f}", ha='center', va='bottom')

pip install catboost

from catboost import CatBoostClassifier

from catboost import CatBoostClassifier

classifiers = [
    ('Bagging (DT)', BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=15), n_estimators=150, random_state=42)),
    ('AdaBoost (DT)', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=15), n_estimators=150, random_state=42)),
    ('Random Forest', RandomForestClassifier(n_estimators=150, max_depth=15, random_state=42)),
    ('Stacking', StackingClassifier(estimators=base_classifiers, final_estimator=LogisticRegression())),
    ('Voting', VotingClassifier(estimators=base_classifiers, voting='soft')),
    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=150, learning_rate=1.0, max_depth=15, random_state=42)),
    ('Extra Trees', ExtraTreesClassifier(n_estimators=150, max_depth=15, random_state=42)),
    ('CatBoost', CatBoostClassifier(random_state=42, verbose=0))  # Set verbose=0 to suppress CatBoost output
]

accuracy_scores1 = []
precision_scores1 = []
recall_scores1 = []
f1_scores1 = []

for clf_name, clf in classifiers:
    # Train classifier
    clf.fit(X_train_pca, y_train)

    # Predictions
    y_pred = clf.predict(X_test_pca)

    # Evaluate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    # Append metrics to lists
    accuracy_scores1.append(accuracy)
    precision_scores1.append(precision)
    recall_scores1.append(recall)
    f1_scores1.append(f1)

    # Print results
    print(f"Classifier: {clf_name}")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")
    print("\n")

clf_accuracy = {}
for clf_name, clf in classifiers:
    clf.fit(X_train_pca, y_train)
    y_pred = clf.predict(X_test_pca)
    clf_accuracy[clf_name] = accuracy_score(y_test, y_pred)

plt.figure(figsize=(15, 5))

clf_colors = ['cyan', 'magenta', 'yellow', 'pink', 'brown', 'gray', 'lime', 'black']
clf_x = np.arange(len(classifiers)) + len(base_accuracy) + 1
plt.bar(clf_x, clf_accuracy.values(), color=clf_colors)
plt.xticks(clf_x, clf_accuracy.keys())
for i, acc in enumerate(clf_accuracy.values()):
    plt.text(i + len(base_accuracy) + 1, acc + 0.01, f"{acc:.4f}", ha='center', va='bottom')

plt.figure(figsize=(5, 5))

best_base = max(base_accuracy, key=base_accuracy.get)
best_clf = max(clf_accuracy, key=clf_accuracy.get)

bar_width = 0.2

# Plot the bars with the specified width
plt.bar([best_base, best_clf], [base_accuracy[best_base], clf_accuracy[best_clf]], color=['blue', 'orange'], width=bar_width)

# Connect the bars with a line
plt.plot([best_base, best_clf], [base_accuracy[best_base], clf_accuracy[best_clf]], marker='o', linestyle='-', color='gray')

# Display the model with the highest accuracy below each bar
#plt.text(best_base, min(base_accuracy[best_base], clf_accuracy[best_clf]) - 0.02, f"{best_base}: {base_accuracy[best_base]:.4f}", ha='center', va='bottom')
#plt.text(best_clf, min(base_accuracy[best_base], clf_accuracy[best_clf]) - 0.02, f"{best_clf}: {clf_accuracy[best_clf]:.4f}", ha='center', va='bottom')

plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Comparing the Best Models')
# plt.show()  # Commenting out plt.show()

# Add a line of text below the graph
print(f"After comparing, the best model is: {best_base if base_accuracy[best_base] > clf_accuracy[best_clf] else best_clf}")

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Calculate ROC curve and AUC for base classifiers
base_classifier_roc_curves = {}
base_classifier_auc_scores = {}

for clf_name, clf in base_classifiers:
    y_pred_proba = clf.predict_proba(X_test_pca)[:, 1]  # Probability estimates of the positive class
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    base_classifier_roc_curves[clf_name] = (fpr, tpr)
    base_classifier_auc_scores[clf_name] = roc_auc_score(y_test, y_pred_proba)

# Calculate ROC curve and AUC for classifiers
classifier_roc_curves = {}
classifier_auc_scores = {}

for clf_name, clf in classifiers:
    y_pred_proba = clf.predict_proba(X_test_pca)[:, 1]  # Probability estimates of the positive class
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    classifier_roc_curves[clf_name] = (fpr, tpr)
    classifier_auc_scores[clf_name] = roc_auc_score(y_test, y_pred_proba)

# Plot ROC curves for base classifiers
plt.figure(figsize=(10, 6))

for clf_name, (fpr, tpr) in base_classifier_roc_curves.items():
    plt.plot(fpr, tpr, label=f'{clf_name} (AUC = {base_classifier_auc_scores[clf_name]:.4f})')

plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Base Classifiers')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Plot ROC curves for classifiers
plt.figure(figsize=(10, 6))

for clf_name, (fpr, tpr) in classifier_roc_curves.items():
    plt.plot(fpr, tpr, label=f'{clf_name} (AUC = {classifier_auc_scores[clf_name]:.4f})')

plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Classifiers')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

metrics_df = pd.DataFrame({
    'Model': ['LR', 'KNN', 'DT', 'MLP', 'SVM', 'Bagging (DT)', 'AdaBoost (DT)', 'Random Forest', 'Stacking', 'Voting', 'Gradient Boosting', 'Extra Trees', 'CatBoost'],
    'Accuracy': accuracy_scores + accuracy_scores1,
    'Precision': precision_scores + precision_scores1,
    'Recall': recall_scores + recall_scores1,
    'F1 Score': f1_scores + f1_scores1
})

# Set the 'Model' column as the index
metrics_df.set_index('Model', inplace=True)

# Display the DataFrame
print("Metrics for Base Learners and Ensemble Learners:")
print(metrics_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Define the data
models = ['LR', 'KNN', 'DT', 'MLP', 'SVM', 'Bagging (DT)', 'AdaBoost (DT)', 'Random Forest', 'Stacking', 'Voting', 'Gradient Boosting', 'Extra Trees', 'CatBoost']
accuracy = [0.867347, 0.862245, 0.809524, 0.857143, 0.867347, 0.874150, 0.850340, 0.869048, 0.865646, 0.865646, 0.826531, 0.862245, 0.870748]

# Create DataFrame
import pandas as pd
data = {'Model': models, 'Accuracy': accuracy}
df_accuracy = pd.DataFrame(data)
df_accuracy.set_index('Model', inplace=True)

# Plotting
plt.figure(figsize=(10, 6))
sns.barplot(x=df_accuracy.index, y='Accuracy', data=df_accuracy, palette='viridis')
plt.title('Comparison of Model Accuracy')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Find the best model based on accuracy
best_model = df_accuracy.idxmax(axis=0)['Accuracy']
print("The best model based on accuracy is:", best_model)

from sklearn.model_selection import cross_val_score

# Cross-validation for base classifiers
for clf_name, clf in base_classifiers:
    cv_scores = cross_val_score(clf, X_train_pca, y_train, cv=5, scoring='accuracy')
    print(f"Cross-validation scores for {clf_name}: {cv_scores}")
    print(f"Mean cross-validation accuracy for {clf_name}: {np.mean(cv_scores)}")
    print("\n")

# Cross-validation for ensemble classifiers
for clf_name, clf in classifiers:
    cv_scores = cross_val_score(clf, X_train_pca, y_train, cv=5, scoring='accuracy')
    print(f"Cross-validation scores for {clf_name}: {cv_scores}")
    print(f"Mean cross-validation accuracy for {clf_name}: {np.mean(cv_scores)}")
    print("\n")